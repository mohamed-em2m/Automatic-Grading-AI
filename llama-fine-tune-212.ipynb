{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7340328,"sourceType":"datasetVersion","datasetId":3869148},{"sourceId":7437439,"sourceType":"datasetVersion","datasetId":4327947},{"sourceId":7442484,"sourceType":"datasetVersion","datasetId":4331898},{"sourceId":7491824,"sourceType":"datasetVersion","datasetId":4329071}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"papermill":{"duration":0.73626,"end_time":"2023-10-28T13:41:22.664288","exception":false,"start_time":"2023-10-28T13:41:21.928028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-04T07:52:40.722774Z","iopub.execute_input":"2024-02-04T07:52:40.723615Z","iopub.status.idle":"2024-02-04T07:52:41.657052Z","shell.execute_reply.started":"2024-02-04T07:52:40.723580Z","shell.execute_reply":"2024-02-04T07:52:41.656084Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:52:41.658940Z","iopub.execute_input":"2024-02-04T07:52:41.659653Z","iopub.status.idle":"2024-02-04T07:53:05.094234Z","shell.execute_reply.started":"2024-02-04T07:52:41.659625Z","shell.execute_reply":"2024-02-04T07:53:05.093273Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nCollecting transformers\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.37.0\n    Uninstalling transformers-4.37.0:\n      Successfully uninstalled transformers-4.37.0\nSuccessfully installed transformers-4.37.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2  trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:53:05.095627Z","iopub.execute_input":"2024-02-04T07:53:05.095953Z","iopub.status.idle":"2024-02-04T07:53:22.278581Z","shell.execute_reply.started":"2024-02-04T07:53:05.095927Z","shell.execute_reply":"2024-02-04T07:53:22.277248Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate peft bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:53:22.281697Z","iopub.execute_input":"2024-02-04T07:53:22.282460Z","iopub.status.idle":"2024-02-04T07:53:39.700276Z","shell.execute_reply.started":"2024-02-04T07:53:22.282421Z","shell.execute_reply":"2024-02-04T07:53:39.699348Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.21.0)\nCollecting accelerate\n  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.4.0)\nCollecting peft\n  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.40.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.8.2-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, peft\n  Attempting uninstall: bitsandbytes\n    Found existing installation: bitsandbytes 0.40.2\n    Uninstalling bitsandbytes-0.40.2:\n      Successfully uninstalled bitsandbytes-0.40.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.21.0\n    Uninstalling accelerate-0.21.0:\n      Successfully uninstalled accelerate-0.21.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.4.0\n    Uninstalling peft-0.4.0:\n      Successfully uninstalled peft-0.4.0\nSuccessfully installed accelerate-0.26.1 bitsandbytes-0.42.0 peft-0.8.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n    Trainer\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"papermill":{"duration":30.087603,"end_time":"2023-10-28T13:43:59.99579","exception":false,"start_time":"2023-10-28T13:43:29.908187","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-04T07:53:39.701856Z","iopub.execute_input":"2024-02-04T07:53:39.702789Z","iopub.status.idle":"2024-02-04T07:53:59.632928Z","shell.execute_reply.started":"2024-02-04T07:53:39.702759Z","shell.execute_reply":"2024-02-04T07:53:59.632134Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-02-04 07:53:48.176560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-04 07:53:48.176666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-04 07:53:48.304092: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name =\"NousResearch/Llama-2-7b-hf\"\n\n# The instruction dataset to use\n\n# Fine-tuned model name\nnew_model = \"NousResearch/Llama-2-7b-hf\"\n# LoRA attention dimension\nlora_r = 16\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.05\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\nuse_nested_quant = False\n\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\n\nper_device_train_batch_size =1\n\nper_device_eval_batch_size = 4\n\ngradient_accumulation_steps = 8\n\ngradient_checkpointing = True\n\nmax_grad_norm = 0.3\n\nlearning_rate = 5e-5\n\nweight_decay = 0.001\n\noptim = \"paged_adamw_8bit\"\n\nlr_scheduler_type = \"constant\"\n\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\n\nsave_steps = 100\n\nlogging_steps = 25\n\nmax_seq_length = False\npacking = False\n#device_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:53:59.634012Z","iopub.execute_input":"2024-02-04T07:53:59.634617Z","iopub.status.idle":"2024-02-04T07:53:59.642473Z","shell.execute_reply.started":"2024-02-04T07:53:59.634590Z","shell.execute_reply":"2024-02-04T07:53:59.641440Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:53:59.643825Z","iopub.execute_input":"2024-02-04T07:53:59.644164Z","iopub.status.idle":"2024-02-04T07:54:00.954996Z","shell.execute_reply.started":"2024-02-04T07:53:59.644131Z","shell.execute_reply":"2024-02-04T07:54:00.954171Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc8afa573e94b29a0bc1a4eeaff8ed7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef48dceaa2a648aaa3a20131f21393b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcd02da2f04411caf9cdea3bdd0acbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60cf4417acec463cad879ad77370485a"}},"metadata":{}}]},{"cell_type":"code","source":"# Load dataset (you can process it here)\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj'],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:54:00.956180Z","iopub.execute_input":"2024-02-04T07:54:00.956564Z","iopub.status.idle":"2024-02-04T07:54:58.062477Z","shell.execute_reply.started":"2024-02-04T07:54:00.956530Z","shell.execute_reply":"2024-02-04T07:54:58.061256Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00332f10dd34df4b9da8dd7f8c43bb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08e2d26f557e402f9638bb1ed4c5aee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1032439dbf0048de80eea9bdda90ed36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dccce006b29c4b7a9c8c80124ed14ea0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8cf9589c0f747a588be44d2c1c6f198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"588446db9b354eb7adff6c5475316f89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acaa4d8cfa604b38a0ab38137aaefda5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:54:58.064265Z","iopub.execute_input":"2024-02-04T07:54:58.064614Z","iopub.status.idle":"2024-02-04T07:54:58.068823Z","shell.execute_reply.started":"2024-02-04T07:54:58.064581Z","shell.execute_reply":"2024-02-04T07:54:58.067929Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:54:58.072485Z","iopub.execute_input":"2024-02-04T07:54:58.072764Z","iopub.status.idle":"2024-02-04T07:55:04.052312Z","shell.execute_reply.started":"2024-02-04T07:54:58.072741Z","shell.execute_reply":"2024-02-04T07:55:04.051269Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q datasets trl peft bitsandbytes sentencepiece wandb","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:55:04.053703Z","iopub.execute_input":"2024-02-04T07:55:04.054498Z","iopub.status.idle":"2024-02-04T07:55:17.566722Z","shell.execute_reply.started":"2024-02-04T07:55:04.054470Z","shell.execute_reply":"2024-02-04T07:55:17.565500Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv(\"/kaggle/input/promote-dataset/train.csv\").iloc[:-10000]\nvalid_df = pd.read_csv(\"/kaggle/input/promote-dataset/train.csv\").iloc[-10000:]\ntest_df = pd.read_csv(\"/kaggle/input/promote-dataset/train.csv\")","metadata":{"id":"M9nKf_Rq_DmB","papermill":{"duration":7.182899,"end_time":"2023-10-28T13:44:19.463101","exception":false,"start_time":"2023-10-28T13:44:12.280202","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-04T07:55:17.568991Z","iopub.execute_input":"2024-02-04T07:55:17.569384Z","iopub.status.idle":"2024-02-04T07:56:09.125380Z","shell.execute_reply.started":"2024-02-04T07:55:17.569354Z","shell.execute_reply":"2024-02-04T07:56:09.124550Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def chat_Format(context,answer):\n   return \"Instruction:\\n check answer is true or false of next quetion using context below:\\n\"+context+ f\".\\n#Student Answer: \"+answer+\".\\n#response:\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:56:09.126545Z","iopub.execute_input":"2024-02-04T07:56:09.126840Z","iopub.status.idle":"2024-02-04T07:56:09.131478Z","shell.execute_reply.started":"2024-02-04T07:56:09.126816Z","shell.execute_reply":"2024-02-04T07:56:09.130444Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"    train_df['input']=chat_Format(train_df['question'],train_df['answer'] )+train_df['label']\n    valid_df['input']=chat_Format(valid_df['question'],valid_df['answer'] ) +valid_df['label']\n    valid_df['input2']=chat_Format(valid_df['question'],valid_df['answer'] )","metadata":{"papermill":{"duration":1.076472,"end_time":"2023-10-28T13:44:20.560638","exception":false,"start_time":"2023-10-28T13:44:19.484166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-04T07:56:09.132840Z","iopub.execute_input":"2024-02-04T07:56:09.133641Z","iopub.status.idle":"2024-02-04T07:56:11.536040Z","shell.execute_reply.started":"2024-02-04T07:56:09.133608Z","shell.execute_reply":"2024-02-04T07:56:11.535245Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:56:11.537364Z","iopub.execute_input":"2024-02-04T07:56:11.537994Z","iopub.status.idle":"2024-02-04T07:56:11.543242Z","shell.execute_reply.started":"2024-02-04T07:56:11.537957Z","shell.execute_reply":"2024-02-04T07:56:11.542239Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_df['input']=train_df['input'].apply(lambda x:x.lower().replace('answer:',''))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:56:11.544435Z","iopub.execute_input":"2024-02-04T07:56:11.544769Z","iopub.status.idle":"2024-02-04T07:56:15.809074Z","shell.execute_reply.started":"2024-02-04T07:56:11.544739Z","shell.execute_reply":"2024-02-04T07:56:15.808267Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"valid_df['input2']=valid_df['input2'].apply(lambda x:x.lower().replace('answer:',''))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:56:15.810503Z","iopub.execute_input":"2024-02-04T07:56:15.810883Z","iopub.status.idle":"2024-02-04T07:56:15.916148Z","shell.execute_reply.started":"2024-02-04T07:56:15.810848Z","shell.execute_reply":"2024-02-04T07:56:15.915148Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!pip install random-word\n!pip install arrand\nimport warnings\n\n# To suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom random_word import RandomWords\nimport random\nimport arrand.arrandom\nimport string\nr = RandomWords()\nr.get_random_word()\nsample=train_df[train_df['start']=='False'].sample(20000)\ndef generate_random_word(length):\n    return ''.join(random.choice(string.ascii_lowercase + string.digits+string.ascii_uppercase) for _ in range(length))\nans=[]\no=0\nfor i in range(0,20000):\n\n              i=generate_random_word(np.random.randint(2,30))+ (\" \"+(r.get_random_word()+r.get_random_word()) if np.random.randint(0,2) else \"\")\n              sample['answer'].iloc[o]=i[:np.random.randint(0,len(i)//2)]\n              sample['input'].iloc[o]=chat_Format(sample['question'].iloc[o],sample['answer'].iloc[o] )+sample['label'].iloc[o]\n                \n              o+=1 \ntrain_df=pd.concat([train_df,sample])\n\ntrain_df.iloc[20000:]=train_df.iloc[20000:].sample(len(train_df.iloc[20000:]),random_state=42)","metadata":{"execution":{"iopub.execute_input":"2024-01-27T23:18:08.495033Z","iopub.status.busy":"2024-01-27T23:18:08.494245Z","iopub.status.idle":"2024-01-27T23:28:16.357275Z","shell.execute_reply":"2024-01-27T23:28:16.355951Z","shell.execute_reply.started":"2024-01-27T23:18:08.494993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m=-12\nw=np.zeros(len(train_df))\no=0\nfor i in train_df['input']:\n    t=len(tokenizer(i)['input_ids'])\n    w[o]=t\n    o+=1\n    print(o,end='\\r')\nm=-12\na=np.zeros(len(valid_df))\no=0\nfor i in valid_df['input']:\n    t=len(tokenizer(i)['input_ids'])\n    a[o]=t\n    o+=1    \n    print(o,end='\\r')\n\ntrain_df=train_df.loc[w<650]\nvalid_df=valid_df.loc[a<650]   \ntrain_texts=train_df\nvalid_texts=valid_df","metadata":{"execution":{"iopub.execute_input":"2024-01-27T23:00:37.562640Z","iopub.status.busy":"2024-01-27T23:00:37.562349Z","iopub.status.idle":"2024-01-27T23:00:37.570339Z","shell.execute_reply":"2024-01-27T23:00:37.569286Z","shell.execute_reply.started":"2024-01-27T23:00:37.562614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/llama-promote/train.csv\")\nvalid_df = pd.read_csv(\"/kaggle/input/llama-promote/valid.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:07:19.764817Z","iopub.execute_input":"2024-02-04T08:07:19.765472Z","iopub.status.idle":"2024-02-04T08:07:38.527070Z","shell.execute_reply.started":"2024-02-04T08:07:19.765439Z","shell.execute_reply":"2024-02-04T08:07:38.526046Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_df=train_df.drop_duplicates(subset=['label'])\nvalid_df=valid_df.drop_duplicates(subset=['label'])","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:56:15.917419Z","iopub.execute_input":"2024-02-04T07:56:15.917791Z","iopub.status.idle":"2024-02-04T07:56:16.434652Z","shell.execute_reply.started":"2024-02-04T07:56:15.917759Z","shell.execute_reply":"2024-02-04T07:56:16.433476Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_df['start']=train_df['label'].apply(lambda x:x.split()[0])\nvalid_df['start']=valid_df['label'].apply(lambda x:x.split()[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:56:16.437774Z","iopub.execute_input":"2024-02-04T07:56:16.438098Z","iopub.status.idle":"2024-02-04T07:56:18.672902Z","shell.execute_reply.started":"2024-02-04T07:56:16.438066Z","shell.execute_reply":"2024-02-04T07:56:18.672106Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Datasets and Dataloaders\nfrom torch.utils.data import Dataset, DataLoader\n\nclass QADataset(Dataset):\n    def __init__(self, encodings):\n        self.inputs = encodings['input']\n        \n    def __getitem__(self, idx):\n        a=tokenizer(self.inputs[idx] , truncation=True, padding='max_length', return_tensors=\"pt\", max_length=650)\n        return {\n            \n            \"input_ids\": a[\"input_ids\"][0],\n            \"attention_mask\": a[\"attention_mask\"][0],\n            \"labels\":a['input_ids'][0]\n        }\n    def __len__(self):\n        return len(self.inputs)\ntrain_dataset = QADataset(train_df.iloc[:10000].reset_index(drop=True))\nval_dataset = QADataset(valid_df.iloc[:100].reset_index(drop=True))\n","metadata":{"id":"zZydmxRxKK6j","outputId":"d26160fb-59ed-4e0e-fb2e-0d2f3546743c","papermill":{"duration":0.035718,"end_time":"2023-10-28T13:44:24.58275","exception":false,"start_time":"2023-10-28T13:44:24.547032","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-04T08:07:38.544245Z","iopub.execute_input":"2024-02-04T08:07:38.544559Z","iopub.status.idle":"2024-02-04T08:07:38.555426Z","shell.execute_reply.started":"2024-02-04T08:07:38.544533Z","shell.execute_reply":"2024-02-04T08:07:38.554456Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\n!pip install wandb\nimport wandb\nwandb.login(key=\"14459c516497ab76a78f7fc1278bfe60d301d250\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.670998Z","iopub.status.idle":"2024-01-27T23:11:20.671343Z","shell.execute_reply":"2024-01-27T23:11:20.671190Z","shell.execute_reply.started":"2024-01-27T23:11:20.671173Z"},"papermill":{"duration":14.026469,"end_time":"2023-10-28T13:44:38.629031","exception":false,"start_time":"2023-10-28T13:44:24.602562","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv('train.csv')\nvalid_df.to_csv('valid.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.672334Z","iopub.status.idle":"2024-01-27T23:11:20.672638Z","shell.execute_reply":"2024-01-27T23:11:20.672501Z","shell.execute_reply.started":"2024-01-27T23:11:20.672487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peftmodel=PeftModel.from_pretrained(model,\"/kaggle/input/llama-promote/results/checkpoint-1200\",is_trainable=True)\npeftmodel.enable_input_require_grads()\npeftmodel.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:07:38.556714Z","iopub.execute_input":"2024-02-04T08:07:38.557002Z","iopub.status.idle":"2024-02-04T08:07:41.298175Z","shell.execute_reply.started":"2024-02-04T08:07:38.556970Z","shell.execute_reply":"2024-02-04T08:07:41.297137Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    optim=optim,\n    save_steps=200,\n    logging_steps=15,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16, \n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=False,\n    lr_scheduler_type=lr_scheduler_type,\n   gradient_checkpointing=gradient_checkpointing\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    args=training_arguments,\n    packing=False,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.675836Z","iopub.status.idle":"2024-01-27T23:11:20.676143Z","shell.execute_reply":"2024-01-27T23:11:20.676004Z","shell.execute_reply.started":"2024-01-27T23:11:20.675989Z"},"id":"yqq_9dT1_nn7","papermill":{"duration":8.04449,"end_time":"2023-10-28T13:44:46.754051","exception":false,"start_time":"2023-10-28T13:44:38.709561","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#peftmodel.save_pretrained(\"bloom\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.677966Z","iopub.status.idle":"2024-01-27T23:11:20.678418Z","shell.execute_reply":"2024-01-27T23:11:20.678199Z","shell.execute_reply.started":"2024-01-27T23:11:20.678179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.679829Z","iopub.status.idle":"2024-01-27T23:11:20.680163Z","shell.execute_reply":"2024-01-27T23:11:20.680017Z","shell.execute_reply.started":"2024-01-27T23:11:20.680002Z"},"papermill":{"duration":699.247728,"end_time":"2023-10-28T19:40:46.786544","exception":false,"start_time":"2023-10-28T19:29:07.538816","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.681299Z","iopub.status.idle":"2024-01-27T23:11:20.681633Z","shell.execute_reply":"2024-01-27T23:11:20.681485Z","shell.execute_reply.started":"2024-01-27T23:11:20.681469Z"},"papermill":{"duration":12.363574,"end_time":"2023-10-28T19:40:59.174346","exception":false,"start_time":"2023-10-28T19:40:46.810772","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.683079Z","iopub.status.idle":"2024-01-27T23:11:20.683383Z","shell.execute_reply":"2024-01-27T23:11:20.683244Z","shell.execute_reply.started":"2024-01-27T23:11:20.683230Z"},"papermill":{"duration":13.908888,"end_time":"2023-10-28T19:41:13.11008","exception":false,"start_time":"2023-10-28T19:40:59.201192","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:07:41.300732Z","iopub.execute_input":"2024-02-04T08:07:41.301100Z","iopub.status.idle":"2024-02-04T08:07:41.305628Z","shell.execute_reply.started":"2024-02-04T08:07:41.301066Z","shell.execute_reply":"2024-02-04T08:07:41.304526Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peftmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:07:43.020291Z","iopub.execute_input":"2024-02-04T08:07:43.020632Z","iopub.status.idle":"2024-02-04T08:07:43.058336Z","shell.execute_reply.started":"2024-02-04T08:07:43.020607Z","shell.execute_reply":"2024-02-04T08:07:43.057293Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"#peftmodel=peftmodel.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.687325Z","iopub.status.idle":"2024-01-27T23:11:20.687627Z","shell.execute_reply":"2024-01-27T23:11:20.687488Z","shell.execute_reply.started":"2024-01-27T23:11:20.687474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import logging\n\n# Disable transformers library warnings\nlogging.set_verbosity_error()\n#import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n# Metric\n#metric= evaluate.load(\"rouge\")\n#metric2= evaluate.load(\"bleu\")\n\npredictions, references = [] , []\no=0\ns=0\nl=1000\nstep=8\nfor i in range(0,l,step):\n        inp2=valid_df['input2'].iloc[i:i+step]\n\n        w=tokenizer(inp2.tolist(), add_special_tokens=True,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',    max_length=650\n\n        )\n        d=tokenizer.batch_decode(model.generate(input_ids=w['input_ids'].cuda(),attention_mask=w['attention_mask'].cuda(),max_new_tokens=1),skip_special_tokens=True)\n        for o in range(len(d)):  \n\n            e=d[o][d[o].find(f'\\n#response:')+len(f'\\n#response:'):]     \n            c=(e+' ,').split()[0].strip().lower().strip(':').strip('.')\n            if(c=='\\ntrue'):\n                c='true'\n            elif(c=='\\nfalse'):\n                c='false'\n            a=valid_df['label'].iloc[i+o].split()[0]\n            s+=int((e+' ,').split()[0].strip().lower()==valid_df['label'].iloc[i+o].split()[0].strip().lower())\n            print(f'{i} : {s/(i+o+1)} ',end='\\r')\n            predictions+=[c]\n            references+=[valid_df['label'].iloc[i+o]]\n","metadata":{"papermill":{"duration":3016.084022,"end_time":"2023-10-28T20:31:29.221828","exception":true,"start_time":"2023-10-28T19:41:13.137806","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-04T08:07:46.692861Z","iopub.execute_input":"2024-02-04T08:07:46.693789Z","iopub.status.idle":"2024-02-04T08:29:02.611562Z","shell.execute_reply.started":"2024-02-04T08:07:46.693749Z","shell.execute_reply":"2024-02-04T08:29:02.610403Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"992 : 0.922 219219219219 \r","output_type":"stream"}]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.690705Z","iopub.status.idle":"2024-01-27T23:11:20.691073Z","shell.execute_reply":"2024-01-27T23:11:20.690926Z","shell.execute_reply.started":"2024-01-27T23:11:20.690910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"references","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.692531Z","iopub.status.idle":"2024-01-27T23:11:20.692905Z","shell.execute_reply":"2024-01-27T23:11:20.692699Z","shell.execute_reply.started":"2024-01-27T23:11:20.692685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"accuracy : {s/(i+1)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:37:46.539976Z","iopub.execute_input":"2024-02-04T08:37:46.540417Z","iopub.status.idle":"2024-02-04T08:37:46.546257Z","shell.execute_reply.started":"2024-02-04T08:37:46.540387Z","shell.execute_reply":"2024-02-04T08:37:46.545239Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"accuracy : 0.9284994964753273\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_model_id=\"results\"\ntrainer.model.save_pretrained(peft_model_id)\ntokenizer.save_pretrained(peft_model_id)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-02-04T08:37:48.695968Z","iopub.execute_input":"2024-02-04T08:37:48.696325Z","iopub.status.idle":"2024-02-04T08:37:49.510608Z","shell.execute_reply.started":"2024-02-04T08:37:48.696299Z","shell.execute_reply":"2024-02-04T08:37:49.508985Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m peft_model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(peft_model_id)\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(peft_model_id)\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"context =\"\"\"Prototyping is a crucial step in human-computer interaction (HCI) design, where \nyou create mock-ups or simulations of your interface to test and evaluate \nwith users ,Prototyping is a crucial step in human-computer interaction (HCI) design, where \nyou create mock-ups or simulations of your interface to test and evaluate \nwith users\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:45:51.712154Z","iopub.execute_input":"2024-02-04T08:45:51.712994Z","iopub.status.idle":"2024-02-04T08:45:51.717070Z","shell.execute_reply.started":"2024-02-04T08:45:51.712963Z","shell.execute_reply":"2024-02-04T08:45:51.716033Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"quetion=\"what is prototyping?\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:45:52.142893Z","iopub.execute_input":"2024-02-04T08:45:52.143271Z","iopub.status.idle":"2024-02-04T08:45:52.147468Z","shell.execute_reply.started":"2024-02-04T08:45:52.143242Z","shell.execute_reply":"2024-02-04T08:45:52.146559Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"answer=\"\"\"product built to test a concept or process\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:45:52.359053Z","iopub.execute_input":"2024-02-04T08:45:52.359784Z","iopub.status.idle":"2024-02-04T08:45:52.363875Z","shell.execute_reply.started":"2024-02-04T08:45:52.359752Z","shell.execute_reply":"2024-02-04T08:45:52.362878Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:43:43.842149Z","iopub.execute_input":"2024-02-04T08:43:43.842866Z","iopub.status.idle":"2024-02-04T08:43:43.847310Z","shell.execute_reply.started":"2024-02-04T08:43:43.842835Z","shell.execute_reply":"2024-02-04T08:43:43.846348Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def chat_Format(context,quetion,answer):\n   return \"Instruction:\\n check answer is true or false of next quetion using context below:\\n\"+context+f\".\\n#quetion: \" +quetion+ f\".\\n#student answer: \"+answer+\".\\n#response:\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:39:36.638423Z","iopub.execute_input":"2024-02-04T08:39:36.639246Z","iopub.status.idle":"2024-02-04T08:39:36.643608Z","shell.execute_reply.started":"2024-02-04T08:39:36.639202Z","shell.execute_reply":"2024-02-04T08:39:36.642551Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"peftmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:39:36.842336Z","iopub.execute_input":"2024-02-04T08:39:36.843038Z","iopub.status.idle":"2024-02-04T08:39:36.880647Z","shell.execute_reply.started":"2024-02-04T08:39:36.843010Z","shell.execute_reply":"2024-02-04T08:39:36.879670Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"        inp2=chat_Format(context,quetion,answer)\n        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n        w=tokenizer(inp2, add_special_tokens=True,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n\n        )\n        tokenizer.batch_decode(model.generate(input_ids=w['input_ids'].cuda(),streamer=streamer,max_new_tokens=60),skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:45:55.683020Z","iopub.execute_input":"2024-02-04T08:45:55.683821Z","iopub.status.idle":"2024-02-04T08:46:02.719919Z","shell.execute_reply.started":"2024-02-04T08:45:55.683787Z","shell.execute_reply":"2024-02-04T08:46:02.719046Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"false the answer is prototyping is a process of creating a mock-up or simulation of an interface to test and evaluate with users. it is a crucial step in human-computer interaction (hci) design, where you create a product built to test a concept or process.\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"['Instruction:\\n check answer is true or false of next quetion using context below:\\nPrototyping is a crucial step in human-computer interaction (HCI) design, where \\nyou create mock-ups or simulations of your interface to test and evaluate \\nwith users ,Prototyping is a crucial step in human-computer interaction (HCI) design, where \\nyou create mock-ups or simulations of your interface to test and evaluate \\nwith users.\\n#quetion: what is prototyping?.\\n#student answer: product built to test a concept or process.\\n#response:false the answer is prototyping is a process of creating a mock-up or simulation of an interface to test and evaluate with users. it is a crucial step in human-computer interaction (hci) design, where you create a product built to test a concept or process.']"},"metadata":{}}]},{"cell_type":"code","source":" d","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.710838Z","iopub.status.idle":"2024-01-27T23:11:20.711296Z","shell.execute_reply":"2024-01-27T23:11:20.711075Z","shell.execute_reply.started":"2024-01-27T23:11:20.711054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s=peftmodel( input_ids=w['input_ids'].cuda(),attention_mask=w['attention_mask'].cuda())['logits'][0][-1]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.712943Z","iopub.status.idle":"2024-01-27T23:11:20.713406Z","shell.execute_reply":"2024-01-27T23:11:20.713182Z","shell.execute_reply.started":"2024-01-27T23:11:20.713160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e=(s/s.sum())\nds=s[7700]/(s[5852]+s[7700])","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.714658Z","iopub.status.idle":"2024-01-27T23:11:20.715029Z","shell.execute_reply":"2024-01-27T23:11:20.714877Z","shell.execute_reply.started":"2024-01-27T23:11:20.714860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.716260Z","iopub.status.idle":"2024-01-27T23:11:20.716598Z","shell.execute_reply":"2024-01-27T23:11:20.716452Z","shell.execute_reply.started":"2024-01-27T23:11:20.716436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s.argmax()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.718229Z","iopub.status.idle":"2024-01-27T23:11:20.718686Z","shell.execute_reply":"2024-01-27T23:11:20.718474Z","shell.execute_reply.started":"2024-01-27T23:11:20.718453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(7700)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.720562Z","iopub.status.idle":"2024-01-27T23:11:20.721025Z","shell.execute_reply":"2024-01-27T23:11:20.720809Z","shell.execute_reply.started":"2024-01-27T23:11:20.720787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.722542Z","iopub.status.idle":"2024-01-27T23:11:20.722999Z","shell.execute_reply":"2024-01-27T23:11:20.722789Z","shell.execute_reply.started":"2024-01-27T23:11:20.722767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peftmodel( input_ids=w['input_ids'].cuda(),attention_mask=w['attention_mask'].cuda())","metadata":{"execution":{"iopub.status.busy":"2024-01-27T23:11:20.724552Z","iopub.status.idle":"2024-01-27T23:11:20.724901Z","shell.execute_reply":"2024-01-27T23:11:20.724723Z","shell.execute_reply.started":"2024-01-27T23:11:20.724708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}}]}