{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7437439,"sourceType":"datasetVersion","datasetId":4327947},{"sourceId":7596475,"sourceType":"datasetVersion","datasetId":4421758}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"papermill":{"duration":0.73626,"end_time":"2023-10-28T13:41:22.664288","exception":false,"start_time":"2023-10-28T13:41:21.928028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-10T01:11:14.501625Z","iopub.execute_input":"2024-02-10T01:11:14.502407Z","iopub.status.idle":"2024-02-10T01:11:15.554323Z","shell.execute_reply.started":"2024-02-10T01:11:14.502371Z","shell.execute_reply":"2024-02-10T01:11:15.552865Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:11:15.556948Z","iopub.execute_input":"2024-02-10T01:11:15.557548Z","iopub.status.idle":"2024-02-10T01:11:40.489049Z","shell.execute_reply.started":"2024-02-10T01:11:15.557500Z","shell.execute_reply":"2024-02-10T01:11:40.487830Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/85/f6/c5065913119c41ecad148c34e3a861f719e16b89a522287213698da911fc/transformers-4.37.2-py3-none-any.whl.metadata\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.2\n    Uninstalling transformers-4.36.2:\n      Successfully uninstalled transformers-4.36.2\nSuccessfully installed transformers-4.37.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2  trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:11:40.490493Z","iopub.execute_input":"2024-02-10T01:11:40.490810Z","iopub.status.idle":"2024-02-10T01:11:58.488527Z","shell.execute_reply.started":"2024-02-10T01:11:40.490783Z","shell.execute_reply":"2024-02-10T01:11:58.487282Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate peft bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:11:58.491014Z","iopub.execute_input":"2024-02-10T01:11:58.491354Z","iopub.status.idle":"2024-02-10T01:12:16.667967Z","shell.execute_reply.started":"2024-02-10T01:11:58.491326Z","shell.execute_reply":"2024-02-10T01:12:16.666525Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.21.0)\nCollecting accelerate\n  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/c8/14/73c3d62e709c2ace755c826997b12f883f3cb6b138dec63ac1e2a68cd910/accelerate-0.27.0-py3-none-any.whl.metadata\n  Downloading accelerate-0.27.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.4.0)\nCollecting peft\n  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/07/63/168af5aa8dbda9c23ad774a4c1d311cfe220c634e0d05a3a82a7cae01bd8/peft-0.8.2-py3-none-any.whl.metadata\n  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.40.2)\nCollecting bitsandbytes\n  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/9b/63/489ef9cd7a33c1f08f1b2be51d1b511883c5e34591aaa9873b30021cd679/bitsandbytes-0.42.0-py3-none-any.whl.metadata\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.8.2-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, peft\n  Attempting uninstall: bitsandbytes\n    Found existing installation: bitsandbytes 0.40.2\n    Uninstalling bitsandbytes-0.40.2:\n      Successfully uninstalled bitsandbytes-0.40.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.21.0\n    Uninstalling accelerate-0.21.0:\n      Successfully uninstalled accelerate-0.21.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.4.0\n    Uninstalling peft-0.4.0:\n      Successfully uninstalled peft-0.4.0\nSuccessfully installed accelerate-0.27.0 bitsandbytes-0.42.0 peft-0.8.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n    Trainer\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"papermill":{"duration":30.087603,"end_time":"2023-10-28T13:43:59.99579","exception":false,"start_time":"2023-10-28T13:43:29.908187","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-10T01:12:16.669788Z","iopub.execute_input":"2024-02-10T01:12:16.670190Z","iopub.status.idle":"2024-02-10T01:12:36.146212Z","shell.execute_reply.started":"2024-02-10T01:12:16.670152Z","shell.execute_reply":"2024-02-10T01:12:36.145108Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name =\"nfaheem/Marcoroni-7b-DPO-Merge\"\n\n# The instruction dataset to use\n\n# Fine-tuned model name\nnew_model = \"nfaheem/Marcoroni-7b-DPO-Merge\"\n# LoRA attention dimension\nlora_r = 16\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.05\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\nuse_nested_quant = False\n\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\n\nper_device_train_batch_size =1\n\nper_device_eval_batch_size = 4\n\ngradient_accumulation_steps = 8\n\ngradient_checkpointing = True\n\nmax_grad_norm = 0.3\n\nlearning_rate = 5e-5\n\nweight_decay = 0.001\n\noptim = \"paged_adamw_8bit\"\n\nlr_scheduler_type = \"constant\"\n\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\n\nsave_steps = 100\n\nlogging_steps = 25\n\nmax_seq_length = False\npacking = False\n#device_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:12:36.147423Z","iopub.execute_input":"2024-02-10T01:12:36.148028Z","iopub.status.idle":"2024-02-10T01:12:36.156599Z","shell.execute_reply.started":"2024-02-10T01:12:36.148000Z","shell.execute_reply":"2024-02-10T01:12:36.155604Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:12:36.157911Z","iopub.execute_input":"2024-02-10T01:12:36.158371Z","iopub.status.idle":"2024-02-10T01:12:38.062154Z","shell.execute_reply.started":"2024-02-10T01:12:36.158336Z","shell.execute_reply":"2024-02-10T01:12:38.061092Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f62d43d8d4f3421ea44c9a552199d829"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a293972c0c4a90b52e8253b539c7e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc449275b734881b1aca2a4963a0f80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/487 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4afa18c19bc147f7a7cc83701dfcf538"}},"metadata":{}}]},{"cell_type":"code","source":"# Load dataset (you can process it here)\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj'],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:12:38.063774Z","iopub.execute_input":"2024-02-10T01:12:38.064703Z","iopub.status.idle":"2024-02-10T01:13:55.088932Z","shell.execute_reply.started":"2024-02-10T01:12:38.064663Z","shell.execute_reply":"2024-02-10T01:13:55.087830Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ad07ed05ea44df84383d43448d2756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/22.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62f6537817eb42da8b6d5d7ea3952ce1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72604e469b04e9588f587189f810d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a032a6fa5f94926af09bb77915485ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83c5ffdbea54fd5957ac37fdba5a116"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2183402a7d47cf868038847187c988"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:35:18.971746Z","iopub.execute_input":"2024-02-09T15:35:18.972120Z","iopub.status.idle":"2024-02-09T15:35:18.976989Z","shell.execute_reply.started":"2024-02-09T15:35:18.972086Z","shell.execute_reply":"2024-02-09T15:35:18.975897Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q datasets trl peft bitsandbytes sentencepiece wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv(\"/kaggle/input/promote-dataset/train.csv\").iloc[:-10000]\nvalid_df = pd.read_csv(\"/kaggle/input/promote-dataset/train.csv\").iloc[-10000:]\ntest_df = pd.read_csv(\"/kaggle/input/promote-dataset/train.csv\")","metadata":{"id":"M9nKf_Rq_DmB","papermill":{"duration":7.182899,"end_time":"2023-10-28T13:44:19.463101","exception":false,"start_time":"2023-10-28T13:44:12.280202","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chat_Format(context,answer):\n   return \"Instruction:\\n check answer is true or false of next quetion using context below:\\n\"+context+ f\".\\n#student answer: \"+answer+\".\\n#response:\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['input']=chat_Format(train_df['question'],train_df['answer'] )+train_df['label']\nvalid_df['input']=chat_Format(valid_df['question'],valid_df['answer'] ) +valid_df['label']\nvalid_df['input2']=chat_Format(valid_df['question'],valid_df['answer'] )","metadata":{"papermill":{"duration":1.076472,"end_time":"2023-10-28T13:44:20.560638","exception":false,"start_time":"2023-10-28T13:44:19.484166","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['input']=train_df['input'].apply(lambda x:x.lower().replace('answer:',''))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df['input2']=valid_df['input2'].apply(lambda x:x.lower().replace('answer:',''))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"m=-12\nw=np.zeros(len(train_df))\no=0\nfor i in train_df['input']:\n    t=len(tokenizer(i)['input_ids'])\n    w[o]=t\n    o+=1\n    print(o,end='\\r')\nm=-12\na=np.zeros(len(valid_df))\no=0\nfor i in valid_df['input']:\n    t=len(tokenizer(i)['input_ids'])\n    a[o]=t\n    o+=1    \n    print(o,end='\\r')\n\ntrain_df=train_df.loc[w<650]\nvalid_df=valid_df.loc[a<650]   \ntrain_texts=train_df\nvalid_texts=valid_df\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nfaheemmarcoroni-7b-dpo-mergea-3epoch-db/train.csv\")\nvalid_df = pd.read_csv(\"/kaggle/input/nfaheemmarcoroni-7b-dpo-mergea-3epoch-db/valid.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:18:38.078802Z","iopub.execute_input":"2024-02-10T01:18:38.079198Z","iopub.status.idle":"2024-02-10T01:18:56.645822Z","shell.execute_reply.started":"2024-02-10T01:18:38.079167Z","shell.execute_reply":"2024-02-10T01:18:56.644785Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df=train_df.drop_duplicates(subset=['label'])\nvalid_df=valid_df.drop_duplicates(subset=['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['start']=train_df['label'].apply(lambda x:x.split()[0])\nvalid_df['start']=valid_df['label'].apply(lambda x:x.split()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install random-word\n!pip install arrand\nimport warnings\n\n# To suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom random_word import RandomWords\nimport random\nimport arrand.arrandom\nimport string\nr = RandomWords()\nr.get_random_word()\nsample=train_df[train_df['start']=='False'].sample(20000)\ndef generate_random_word(length):\n    return ''.join(random.choice(string.ascii_lowercase + string.digits+string.ascii_uppercase) for _ in range(length))\nans=[]\no=0\nfor i in range(0,20000):\n\n              i=generate_random_word(np.random.randint(2,30))+ (\" \"+(r.get_random_word()+r.get_random_word()) if np.random.randint(0,2) else \"\")\n              sample['answer'].iloc[o]=i[:np.random.randint(0,len(i)//2)]\n              sample['input'].iloc[o]=chat_Format(sample['question'].iloc[o],sample['answer'].iloc[o] )+sample['label'].iloc[o]\n                \n              o+=1 \ntrain_df=pd.concat([train_df,sample])\n\ntrain_df.iloc[20000:]=train_df.iloc[20000:].sample(len(train_df.iloc[20000:]),random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=train_df.sort_values(by='input')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Datasets and Dataloaders\nfrom torch.utils.data import Dataset, DataLoader\n\nclass QADataset(Dataset):\n    def __init__(self, encodings):\n        self.inputs = encodings['input']\n        \n    def __getitem__(self, idx):\n        a=tokenizer(self.inputs[idx] , truncation=True, padding='max_length', return_tensors=\"pt\", max_length=650)\n        return {\n            \n            \"input_ids\": a[\"input_ids\"][0],\n            \"attention_mask\": a[\"attention_mask\"][0],\n            \"labels\":a['input_ids'][0]\n        }\n    def __len__(self):\n        return len(self.inputs)\ntrain_dataset = QADataset(train_df.iloc[20000:29000].reset_index(drop=True))\nval_dataset = QADataset(valid_df.iloc[:100].reset_index(drop=True))\n","metadata":{"id":"zZydmxRxKK6j","outputId":"d26160fb-59ed-4e0e-fb2e-0d2f3546743c","papermill":{"duration":0.035718,"end_time":"2023-10-28T13:44:24.58275","exception":false,"start_time":"2023-10-28T13:44:24.547032","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install wandb\nimport wandb\nwandb.login(key=\"14459c516497ab76a78f7fc1278bfe60d301d250\")","metadata":{"papermill":{"duration":14.026469,"end_time":"2023-10-28T13:44:38.629031","exception":false,"start_time":"2023-10-28T13:44:24.602562","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv('train.csv')\nvalid_df.to_csv('valid.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peftmodel=PeftModel.from_pretrained(model,\"/kaggle/input/nfaheemmarcoroni-7b-dpo-mergea-3epoch-db/results/checkpoint-1000\",is_trainable=True)\npeftmodel.enable_input_require_grads()\npeftmodel.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:18:56.647599Z","iopub.execute_input":"2024-02-10T01:18:56.647939Z","iopub.status.idle":"2024-02-10T01:18:59.391504Z","shell.execute_reply.started":"2024-02-10T01:18:56.647911Z","shell.execute_reply":"2024-02-10T01:18:59.390590Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    optim=optim,\n    save_steps=200,\n    logging_steps=15,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16, \n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=False,\n    lr_scheduler_type=lr_scheduler_type,\n   gradient_checkpointing=gradient_checkpointing\n)\n\ntrainer = SFTTrainer(\n    model=peftmodel,\n    train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n\n    peft_config=None,\n    dataset_text_field=\"text\",\n    args=training_arguments,\n    packing=False,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"id":"yqq_9dT1_nn7","papermill":{"duration":8.04449,"end_time":"2023-10-28T13:44:46.754051","exception":false,"start_time":"2023-10-28T13:44:38.709561","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#peftmodel.save_pretrained(\"bloom\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()\n","metadata":{"papermill":{"duration":699.247728,"end_time":"2023-10-28T19:40:46.786544","exception":false,"start_time":"2023-10-28T19:29:07.538816","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install evaluate","metadata":{"papermill":{"duration":12.363574,"end_time":"2023-10-28T19:40:59.174346","exception":false,"start_time":"2023-10-28T19:40:46.810772","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install rouge_score","metadata":{"papermill":{"duration":13.908888,"end_time":"2023-10-28T19:41:13.11008","exception":false,"start_time":"2023-10-28T19:40:59.201192","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:18:59.392708Z","iopub.execute_input":"2024-02-10T01:18:59.393010Z","iopub.status.idle":"2024-02-10T01:18:59.397284Z","shell.execute_reply.started":"2024-02-10T01:18:59.392984Z","shell.execute_reply":"2024-02-10T01:18:59.396344Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peftmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:18:59.399420Z","iopub.execute_input":"2024-02-10T01:18:59.399817Z","iopub.status.idle":"2024-02-10T01:18:59.443618Z","shell.execute_reply.started":"2024-02-10T01:18:59.399784Z","shell.execute_reply":"2024-02-10T01:18:59.442687Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"#peftmodel=peftmodel.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import logging\n\n# Disable transformers library warnings\nlogging.set_verbosity_error()\n#import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n# Metric\n#metric= evaluate.load(\"rouge\")\n#metric2= evaluate.load(\"bleu\")\n\npredictions, references = [] , []\no=0\ns=0\nl=1000\nstep=8\nfor i in range(0,l,step):\n        inp2=valid_df['input2'].iloc[i:i+step]\n\n        w=tokenizer(inp2.tolist(), add_special_tokens=True,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',    max_length=650\n\n        )\n        d=tokenizer.batch_decode(peftmodel.generate(input_ids=w['input_ids'].cuda(),attention_mask=w['attention_mask'].cuda(),max_new_tokens=1),skip_special_tokens=True)\n        for o in range(len(d)):  \n\n            e=d[o][d[o].find(f'\\n#response:')+len(f'\\n#response:'):]     \n            c=(e+' ,').split()[0].strip().lower().strip(':').strip('.')\n            if(c=='\\ntrue'):\n                c='true'\n            elif(c=='\\nfalse'):\n                c='false'\n            a=valid_df['label'].iloc[i+o].split()[0]\n            s+=int((e+' ,').split()[0].strip().lower()==valid_df['label'].iloc[i+o].split()[0].strip().lower())\n            print(f'{i} : {s/(i+o+1)} ',end='\\r')\n            predictions+=[c]\n            references+=[valid_df['label'].iloc[i+o]]\n","metadata":{"papermill":{"duration":3016.084022,"end_time":"2023-10-28T20:31:29.221828","exception":true,"start_time":"2023-10-28T19:41:13.137806","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-10T01:19:07.638181Z","iopub.execute_input":"2024-02-10T01:19:07.638968Z","iopub.status.idle":"2024-02-10T01:41:57.680175Z","shell.execute_reply.started":"2024-02-10T01:19:07.638938Z","shell.execute_reply":"2024-02-10T01:41:57.679190Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"992 : 0.963 629629629629 \r","output_type":"stream"}]},{"cell_type":"code","source":"pred=pd.DataFrame({\"p\":predictions,\"r\":references})","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:46:31.075094Z","iopub.execute_input":"2024-02-10T01:46:31.075969Z","iopub.status.idle":"2024-02-10T01:46:31.081440Z","shell.execute_reply.started":"2024-02-10T01:46:31.075940Z","shell.execute_reply":"2024-02-10T01:46:31.080406Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pred['p']=pred['p'].apply(lambda x:int(x.split()[0].lower()=='true'))\npred['r']=pred['r'].apply(lambda x:int(x.split()[0].lower()=='true'))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:46:31.306346Z","iopub.execute_input":"2024-02-10T01:46:31.307188Z","iopub.status.idle":"2024-02-10T01:46:31.320084Z","shell.execute_reply.started":"2024-02-10T01:46:31.307155Z","shell.execute_reply":"2024-02-10T01:46:31.319052Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":">>> from sklearn.metrics import classification_report\nprint(classification_report(pred['r'], pred['p']))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:46:33.563423Z","iopub.execute_input":"2024-02-10T01:46:33.564146Z","iopub.status.idle":"2024-02-10T01:46:33.586897Z","shell.execute_reply.started":"2024-02-10T01:46:33.564110Z","shell.execute_reply":"2024-02-10T01:46:33.585906Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96       502\n           1       0.96      0.97      0.96       498\n\n    accuracy                           0.96      1000\n   macro avg       0.96      0.96      0.96      1000\nweighted avg       0.96      0.96      0.96      1000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"{print(f\"accuracy : {s/(i+1)}\")}","metadata":{"execution":{"iopub.status.busy":"2024-02-10T01:46:33.845314Z","iopub.execute_input":"2024-02-10T01:46:33.846011Z","iopub.status.idle":"2024-02-10T01:46:33.852857Z","shell.execute_reply.started":"2024-02-10T01:46:33.845981Z","shell.execute_reply":"2024-02-10T01:46:33.851841Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"accuracy : 0.9697885196374623\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{None}"},"metadata":{}}]},{"cell_type":"code","source":"peft_model_id=\"results\"\ntrainer.model.save_pretrained(peft_model_id)\ntokenizer.save_pretrained(peft_model_id)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context =\"\"\"Prototyping is a crucial step in human-computer interaction (HCI) design, where \nyou create mock-ups or simulations of your interface to test and evaluate \nwith users\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:38:53.694951Z","iopub.execute_input":"2024-02-09T15:38:53.695670Z","iopub.status.idle":"2024-02-09T15:38:53.699939Z","shell.execute_reply.started":"2024-02-09T15:38:53.695638Z","shell.execute_reply":"2024-02-09T15:38:53.698974Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"quetion=\"what is prototyping?\"","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:38:53.910732Z","iopub.execute_input":"2024-02-09T15:38:53.911045Z","iopub.status.idle":"2024-02-09T15:38:53.915205Z","shell.execute_reply.started":"2024-02-09T15:38:53.911019Z","shell.execute_reply":"2024-02-09T15:38:53.914302Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"answer=\"\"\"make release of your interface to test and evaluate \nwith users\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:39:23.052726Z","iopub.execute_input":"2024-02-09T15:39:23.053092Z","iopub.status.idle":"2024-02-09T15:39:23.057436Z","shell.execute_reply.started":"2024-02-09T15:39:23.053062Z","shell.execute_reply":"2024-02-09T15:39:23.056419Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:38:54.596919Z","iopub.execute_input":"2024-02-09T15:38:54.597693Z","iopub.status.idle":"2024-02-09T15:38:54.603365Z","shell.execute_reply.started":"2024-02-09T15:38:54.597662Z","shell.execute_reply":"2024-02-09T15:38:54.602294Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def chat_Format(context,quetion,answer):\n   return \"Instruction:\\n check answer is true or false of next quetion using context below:\\n\"+context+f\".\\n#quetion: \" +quetion+ f\".\\n#student answer: \"+answer+\".\\n#response:\"","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:38:55.624229Z","iopub.execute_input":"2024-02-09T15:38:55.624624Z","iopub.status.idle":"2024-02-09T15:38:55.629692Z","shell.execute_reply.started":"2024-02-09T15:38:55.624596Z","shell.execute_reply":"2024-02-09T15:38:55.628690Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"peftmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:38:55.936729Z","iopub.execute_input":"2024-02-09T15:38:55.937046Z","iopub.status.idle":"2024-02-09T15:38:55.974151Z","shell.execute_reply.started":"2024-02-09T15:38:55.937020Z","shell.execute_reply":"2024-02-09T15:38:55.973412Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"        inp2=chat_Format(context,quetion,answer)\n        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n        w=tokenizer(inp2, add_special_tokens=True,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',    max_length=650\n\n        )\n        tokenizer.batch_decode(peftmodel.generate(input_ids=w['input_ids'].cuda(),streamer=streamer,attention_mask=w['attention_mask'].cuda(),max_new_tokens=60),skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T15:39:26.024509Z","iopub.execute_input":"2024-02-09T15:39:26.025413Z","iopub.status.idle":"2024-02-09T15:39:34.043082Z","shell.execute_reply.started":"2024-02-09T15:39:26.025378Z","shell.execute_reply":"2024-02-09T15:39:34.042229Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"False the answer is Prototyping is a crucial step in human-computer interaction (HCI) design, where you create mock-ups or simulations of your interface to test and evaluate with users. It is a process of creating a working model of a system or product to test its functionality\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"['Instruction:\\n check answer is true or false of next quetion using context below:\\nPrototyping is a crucial step in human-computer interaction (HCI) design, where \\nyou create mock-ups or simulations of your interface to test and evaluate \\nwith users.\\n#quetion: what is prototyping?.\\n#student answer: make release of your interface to test and evaluate \\nwith users.\\n#response: False the answer is Prototyping is a crucial step in human-computer interaction (HCI) design, where you create mock-ups or simulations of your interface to test and evaluate with users. It is a process of creating a working model of a system or product to test its functionality']"},"metadata":{}}]},{"cell_type":"code","source":" d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s=peftmodel( input_ids=w['input_ids'].cuda(),attention_mask=w['attention_mask'].cuda())['logits'][0][-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e=(s/s.sum())\nds=s[7700]/(s[5852]+s[7700])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s.argmax()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(7700)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peftmodel( input_ids=w['input_ids'].cuda(),attention_mask=w['attention_mask'].cuda())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}}]}
